---
title             : "How Many Psychologists Use Questionable Research Practices? Estimating the Population Size of Current QRP Users"
shorttitle        : "How Many Psychologists Use QRPs"

author: 
  - name          : "Nicholas W. Fox"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "53 Avenue E, Room 429, Piscataway NJ 08854"
    email         : "nwf7@psych.rutgers.edu"
  - name          : "Nathan Honeycutt"
    affiliation   : "1"
  - name          : "Lee Jussim"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Rutgers University"


author_note: |
  Department of Psychology, Rutgers University, Piscataway NJ 08854

abstract: |
  Psychology has been in crisis.  Over the past 15 years many high-impact research findings have failed to replicate, calling into question their validity. Increased methodological and statistical scrutiny has led to field-wide introspection on how best to produce robust, reproducible, research. One focus has been on the use of questionable research practices.  Previous estimates of the number of researchers who use questionable research practices vary widely, from over 60% to near 10%.  In the current work, the authors produced three estimates of the number of American psychologists who have used questionable research practices in the last 12 months, utilizing direct, indirect, and social network measures of estimation.  We estimate up to 24.40`% of American psychologists have recently used at least one questionable research practice.  These estimates represent the first step in generating actionable interventions to resolve the current replication crisis and increase trust in published research.
  
keywords          : "QRPs, Questionable Research Practices, Replication Crisis, Social Networks"
wordcount         : "4,564"

bibliography      : ["library.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "jou"
output            : papaja::apa6_pdf
---

```{r echo = FALSE}
options(warn = -1, digits = 2, cache = TRUE, scipen=999)

#install packages - remove the hashtag if you need to install these packages, otherwise keep them hashed out.
#install.packages("tidyverse")
#install.packages("boot")
#devtools::install_github("crsh/papaja")
#devtools::install_github("crsh/papaja@devel")
#devtools::install_github("benmarwick/wordcountaddin")
#install.packages("rmarkdown")
#install.packages("kableExtra")
#install.packages("xtable")

#devtools::session_info()
#pandoc_version()

#load packages
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(boot)))
suppressWarnings(suppressPackageStartupMessages(library(papaja)))
suppressWarnings(suppressPackageStartupMessages(library(rmarkdown)))
suppressWarnings(suppressPackageStartupMessages(library(kableExtra)))
suppressWarnings(suppressPackageStartupMessages(library(xtable)))
suppressWarnings(suppressPackageStartupMessages(library(wordcountaddin)))

#this counts all the words in the entire rmd, not including chunks, in line code, etc.
#wordcountaddin::text_stats()


```

```{r echo = FALSE}
#Load in data
direct_estimate_data <- read.csv("direct_estimate_share.csv", header = TRUE)
UCT_data <- read.csv("UCT_estimate_share.csv", header = TRUE)
NSUM_data <- read.csv("NSUM_estimate_share.csv", header = TRUE)
GNSUM_data <- read.csv("GNSUM_estimate_share.csv", header = TRUE)
estimates <- read.csv("qrp_estimates.csv", header = TRUE)
validity <- read.csv("validity_data_share.csv", header = TRUE)
network_sizes_share <- read.csv("network_sizes_share.csv", header = TRUE)
responses <- read.csv("survey_info.csv", header = TRUE)

```

```{r echo = FALSE}
#define bootstrapping functions per estimate

#direct estimate function
DE <- function(d, i){
  d2 <- d[i,]
  return((sum(d2$QRP_USE)/length(d2$QRP_USE)))
}

#UCT estimate function
UCTE <- function(d, i){
  data <- d[i,]
  
  control_items <- data %>%
    filter(survey == 1) %>% 
    summarise(mean = mean(affirmed_items))
  
  sensitive_items <- data %>% 
    filter(survey == 2) %>% 
    summarise(mean = mean(affirmed_items))
  
  return(sensitive_items$mean - control_items$mean)
}

#NSUM estimate function
NSUME <- function(d, i){
  data <- d[i,]
  return(sum(data$y_i) / sum(data$d_i))
}

#GNSUM estimate function
#all the na.rm=TRUE is to deal with data of differing lengths due to different groups of people getting different questions.
GNSUME <- function(d, i){
  data <- d[i,]
  
  #this calculates the NSUM estimate within the bootstrap procedure
  NSUM = sum(data$y_i, na.rm = TRUE) / sum(data$d_i, na.rm = TRUE)
  
  QRP_network <- data %>%
    filter(population == "QRP") %>% 
    summarise(mean = mean(network_size), na.rm = TRUE)
  
  general_network <- data %>%
    filter(population == "general") %>% 
    summarise(mean = mean(network_size), na.rm = TRUE)
  
  #this calculates the delta estimate (comparison of average network size) within the bootstrap procedure
  delta = (QRP_network$mean/general_network$mean)
  
  #Counts number of "I know about their QRP usage, They know about my QRP usage"
  IK_TK <- data %>% 
    filter(quadrant == "IK_TK") %>% 
    summarise(sum = sum(count), na.rm = TRUE)
  
  #Counts number of "I don't know about their QRP usage, They know about my QRP usage"
  IDK_TK <- data %>% 
    filter(quadrant == "IDK_TK") %>% 
    summarise(sum = sum(count), na.rm = TRUE)
  
  #Counts number of "I don't know about their QRP usage, They don't know about my QRP usage"
  IDK_TDK <- data %>%
    filter(quadrant == "IDK_TDK") %>% 
    summarise(sum = sum(count), na.rm = TRUE)
  
  #Counts number of "I know about their QRP usage, They don't know about my QRP usage"
  IK_TDK <- data %>%
    filter(quadrant == "IK_TDK") %>% 
    summarise(sum = sum(count), na.rm = TRUE)
  
  #this calculates tau (transmission rate) within the bootstrap procedure
  tau = (IK_TK$sum + IDK_TK$sum) / (IK_TK$sum + IDK_TK$sum + IDK_TDK$sum + IK_TDK$sum)

  #this returns the GNSUM estimate, once per replicate
  return(NSUM * (1/delta) * (1/tau))
}

```

```{r echo = FALSE}
#DIRECT ESTIMATE
#sets a reproducible seed for the bootstrapping (626 is what is used for the publication)
set.seed(626)

#create bootstrap with 10,000 replicates
boot_DE <- boot(direct_estimate_data, DE, R=10000)

#create histogram of bootstrapped estimate
#plot(boot_DE)

#generate percentile bootstrapped 95% CIs (and finds names of values within)
boot_DE_ci <- boot.ci(boot.out = boot_DE, type = c("perc"))
#names(boot_DE_ci)

```

```{r echo = FALSE}
#UCT ESTIMATE
#sets a reproducible seed for the bootstrapping (626 is what is used for the publication)
set.seed(626)

#create bootstrap with 10,000 replicates
boot_UCTE <- boot(UCT_data, UCTE, R=10000)

#create histogram of bootstrapped estimate
#plot(boot_UCTE)

#generate percentile bootstrapped 95% CIs
boot_UCTE_ci <- boot.ci(boot.out = boot_UCTE, type = c("perc"))

```

```{r echo = FALSE}
#sets a reproducible seed for the bootstrapping (626 is what is used for the publication)
set.seed(626)

#create bootstrap with 10,000 replicates
boot_NSUME <- boot(NSUM_data, NSUME, R=10000)

#create histogram of bootstrapped estimate
#plot(boot_NSUME)

#generate percentile bootstrapped 95% CIs
boot_NSUME_ci <- boot.ci(boot.out = boot_NSUME, type = c("perc"))

```

```{r echo = FALSE}
#sets a reproducible seed for the bootstrapping (626 is what is used for the publication)
set.seed(626)

#create bootstrap with 10,000 replicates (this might take awhile depending on your comptuer)
boot_GNSUME <- boot(GNSUM_data, GNSUME, R=10000)

#create histogram of bootstrapped estimate
#plot(boot_GNSUME)

#generate percentile bootstrapped 95% CIs
boot_GNSUME_ci <- boot.ci(boot.out = boot_GNSUME, type = c("perc"))

```

```{r echo = FALSE}
#this block generates all the individual numbers used in the text.  There is probably a better way of doing it, but this is what I came up with...

all_people <- responses %>% 
  summarise(people = sum(emails_sent))

bounce_rate <- responses %>% 
  summarise(bounce = sum(emails_bounced))

full_responses <- responses %>% 
  summarise(full = sum(full_responses))

partial_responses <- responses %>% 
  summarise(partial = sum(partial_responses))

removed <- responses %>% 
  summarise(removed = sum(removed_participants))

female <- responses %>% 
  summarise(female = sum(female))

male <- responses %>% 
  summarise(male = sum(male))

DND_gender <- responses %>% 
  summarise(DND_gender = sum(DND_gender))

assistant <- responses %>%
  summarise(assistant = sum(assistant))

associate <- responses %>% 
  summarise(associate = sum(associate))

full <- responses %>% 
  summarise(full = sum(full))

DND_tenure <- responses %>% 
  summarise(DND_tenure = sum(DND_tenure))

DE_solicit <- responses %>% 
  filter(survey == 3) %>% 
  summarise(emails_sent)

DE_received_responses <- responses %>%
  filter(survey == 3) %>%
  summarise(analyzed_participants)

DE_numerator <- direct_estimate_data %>%
  summarise(QRP = sum(QRP_USE))

remaining <- responses %>% 
  filter(survey != 3) %>% 
  summarise(sum = sum(emails_sent))

ILC_participants <- responses %>% 
  filter(survey == 1) %>% 
  summarise(sum = emails_sent)

SLC_participants <- responses %>% 
  filter(survey == 2) %>% 
  summarise(sum = emails_sent)

ILC_average <- UCT_data %>% 
  filter(survey == 1) %>% 
  summarise(mean = mean(affirmed_items))

SLC_average <- UCT_data %>%
  filter(survey == 2) %>%
  summarise(mean = mean(affirmed_items))

ILC_total <- responses %>% 
  filter(survey == 1) %>% 
  summarise(analyzed_participants)

SLC_total <- responses %>% 
  filter(survey == 2) %>% 
  summarise(analyzed_participants)

UCT_total_participants <- responses %>%
  filter(survey != 3) %>%
  summarise(sum = sum(analyzed_participants))

```

```{r echo = FALSE}
#This block generates the correlation between estimated and actual group sizes for our GNSUM validity

validity_correlation <- with(validity, cor(method = "pearson", estimate, actual))

```

```{r echo = FALSE}
#this is a block of code used to generate the value of delta for the text

#this calculates the estimate for network size per participant
network_sizes <- network_sizes_share %>% 
  group_by(keyID) %>% 
  mutate(d_i = (sum(name_count)/964)*7101) %>% 
  distinct(keyID, direct_estimate, d_i) %>% 
  ungroup()

#average network size for those identifying as QRP users
QRP_network_size <- network_sizes %>% 
  filter(direct_estimate == 1) %>%
  summarise(mean = mean(d_i))

#average network size for general population
general_network_size <- network_sizes %>% 
  filter(direct_estimate == 2) %>% 
  summarise(mean = mean(d_i))

#calculation for delta
delta <- QRP_network_size / general_network_size

```

```{r echo = FALSE}
#this is a block of code from the function inside the GNSUM bootstrapping block used to generate the value of tau for the text.

#Counts number of "I know about their QRP usage, They know about my QRP usage"
IK_TK <- GNSUM_data %>% 
  filter(quadrant == "IK_TK") %>% 
  summarise(sum = sum(count), na.rm = TRUE)
  
#Counts number of "I don't know about their QRP usage, They know about my QRP usage"
IDK_TK <- GNSUM_data %>% 
  filter(quadrant == "IDK_TK") %>% 
  summarise(sum = sum(count), na.rm = TRUE)
  
#Counts number of "I don't know about their QRP usage, They don't know about my QRP usage"
IDK_TDK <- GNSUM_data %>%
  filter(quadrant == "IDK_TDK") %>% 
  summarise(sum = sum(count), na.rm = TRUE)
  
#Counts number of "I know about their QRP usage, They don't know about my QRP usage"
IK_TDK <- GNSUM_data %>%
  filter(quadrant == "IK_TDK") %>% 
  summarise(sum = sum(count), na.rm = TRUE)
  
#this calculates tau (transmission rate) within the bootstrap procedure
tau = (IK_TK$sum + IDK_TK$sum) / (IK_TK$sum + IDK_TK$sum + IDK_TDK$sum + IK_TDK$sum)
  
```

```{r echo = FALSE}
#generates total number of people who did not identify as a QRP user (or were not asked to identify) who provided information about their social network size
SN_responses <- responses %>%
  summarise(total = sum(analyzed_participants)) - direct_estimate_data %>%
  summarise(sum(QRP_USE))

#generates number of QRP users identified
y_i <- NSUM_data %>%  
  summarise(estimate = sum(y_i))

#generates total number of people known throughout all social networks
d_i <- NSUM_data %>%  
  summarise(estimate = sum(d_i))

```

It is the researcher's job to generate theories, test hypotheses, collect and interpret data, interpret results, and to publish findings. This is all done to learn more about the world and how it works. In the course of doing science, the researcher has many decisions to make: How many subjects will I use? How will I operationalize my variables?  What is my population of interest? Should I exclude any data from the analysis?

Each decision point is a "researcher degree of freedom" [@Simmons2011], with the potential of introducing error and bias. Since there is a high level of ambiguity in research, these degrees of freedom can be resolved in different ways.  In reviewing how researchers deal with outlying observations, @Simmons2011 found different research groups made independent decisions on the best course of action.  When researchers cleaned data and removed participants that responded "too fast", some defined this as 2 standard deviations below the mean response speed, some defined it as observations below 200ms, and others removed the fastest 2.5% of respondents.  None of these definitions are inherently an incorrect interpretation of "too fast", which can be a problem: without clear standards in place, this type of flexible decision making can blur the lines between what decision is right, what decision produces the desired result, and what decision is most likely to help get a finding published.

There are many "degrees of freedom" that exploit the gray areas of acceptable practice that may bias research findings [@John2012; @Wicherts2016].  Some examples include trying different ways to score the chosen primary dependent variable, and deciding how to deal with outliers in an ad hoc manner. Ten of these behaviors have been collectively called "questionable research practices" (QRPs) and are typically defined as behaviors during data collection, analysis, and reporting that have the potential to increase false-positive findings in the literature.   While there are many examples of other behaviors that could be considered questionable, these ten stand out as being familiar to most researchers and having been investigated previously [@John2012; @Fiedler2016; @Agnoli2017].  For this study, nine of the ten QRPs were considered (Table \ref{tab:QRPs}). We did not include “fabricated data” (QRP-10) as a questionable research practice as the authors consider this a fraudulent, not questionable, behavior. Not only can QRP use increase the number of false-positive findings (e.g., taking a “non-significant” result and pushing it over a threshold into being "significant"), but using multiple QRPs can also influence the reported effect size of a given finding due to sampling bias and low power [@Button2013].  Thus, QRP use could lead to field-wide interpretations that are not warranted by the data.

```{r QRPs, echo = FALSE, results = "asis"}


QRP_table <- c("1", "Failing to report all of a study's dependent measures", "After measuring baseline anxiety with three different scales, only reporting one", "2", "Collecting more data after looking to see if the results were significant", "Collecting 25 subjects' worth of data, achieving p = 0.07, then recruiting 25 more subjects", "3", "Failing to report all of a study's conditions", "Collecting data on three experimental conditions (no/low/high anxiety), but only reporting two (no/high anxiety)", "4", "Stopping data collection earlier than planned because one found the result one was looking for", "After collecting half the final number of proposed subjects, achieving p < 0.05, then stopping recruitment", "5", "Rounding off p values to achieve significance", "Achieving p = 0.054, then reporting it as p = 0.05 or p < 0.05", "6", "Selectively reporting studies that 'worked'", "Running eight studies, but only reporting the 5 that had significant findings", "7", "Deciding whether to exclude observations after seeing the effect of doing so on the results", "Removing potential outliers based on their effect on the data and no on an a priori guideline", "8", "Reporting unexpected findings as being predicted from the start (a.k.a Hypothesizing After the Results are Known, or HARKing)", "Predicting a relationship between A and B, but finding a relationship between A and C, reporting the A-C relationship was predicted from the start", "9", "Reporting results are unaffected by demographics when actually unsure or not tested", "Reporting no gender differences when gender was not collected or relationship not tested")

QRP_table_formatted <- matrix(QRP_table, ncol = 3, byrow = TRUE) %>% 
  as_tibble()

names(QRP_table_formatted) <- c("Item", "Questionable Research Practice", "Example")

knitr::kable(QRP_table_formatted, format = "latex", table.env='table*', caption = "Questionable Research Practices of interest with examples.", booktabs = T) %>% column_spec(2, width = "20em") %>% 
  column_spec(3, width = "20em") %>% 
  kable_styling(font_size = 9)

```


## Prevalence of questionable research practices
Consider one of the most basic questions to ask about the current replication crisis: How many people are contributing to it?  @John2012 found 63% of psychologists admitted to publishing work without all the dependent measure included (at some point in their academic career).  As articulated by @Simmons2011, this is highly problematic because increasing the number of dependent variables is correlated with an increase in the probability of finding a significant result.  Without reporting all dependent measures, readers are left with a false impression of the rarity or truthfulness of the reported findings.  But, this estimate from  @John2012 was contested by @Fiedler2016. In their conceptual replication that used  differently worded questions, used a different conceptualization of “prevalence”, and tested a German (as opposed to an American) cohort of psychologists, @Fiedler2016 found less than 10% prevalence of the same questionable practice (omitting dependent variables). Furthermore, @Agnoli2017 recently replicated the original @John2012 study in an Italian cohort of psychologists, and found similarly high levels of QRP use (47.9% of respondents had omitted dependent variables).   Concequently, there is no current consensus on the prevalence of QRP use in psychology, nor any indication of how these may be related to the current replication crisis in the field.

Given the inconsistencies in assessing the prevalence of QRP use, the present work sought to expand on this existing literature in several ways.  First, we investigated current QRP users, operationalized as a person who has used at least one of nine QRPs "in the past 12 months".  This puts QRP use into the timeframe of the current replication crisis.  Second, it addressed the larger issue of "prevalence", by defining behaviors performed within a specified time period.  Previous work estimating QRP prevalence has done so over career-long timespans or via estimating frequency of QRP use, both providing limited insight on the current issues in the field.

A third unique contribution of the present research is that it assessed prevalence of QRP use with several starkly different methodologies. One was a direct estimate tightly based on prior research (i.e., [@John2012;@Fiedler2016;@Agnoli2017]): We simply asked researchers to report their own QRP use.

However, two other methods were increasingly different from this straightforward assessment.  One used the unmatched count technique, an indirect estimate aimed at reducing social desirability response bias ([@Arentoft2016] - see Method for details).  A third generated an indirect estimate of QRP use by using social network information from the general population of psychologists [@Salganik2011;@Zhang2010;@Zheng2006;@Jing2014].  Neither the unmatched count technique nor these social network methods required participants to identify as belonging to a potentially stigmatized group (i.e., QRP users), thereby reducing the risk of socially desirable response bias compared to more traditional direct estimates.  While network methods were expected to provide insights into QRP use prevalence, they have yet to be used in psychology.  Thus, this work produced three estimates of QRP use prevalence.

# METHOD
The work detailed in this manuscript was preregistered on May 15th, 2017.  The preregistration can be found at www.osf.io/xu25n and is detailed in the supplemental materials.

## Sample
The frame population was tenured or tenure-track faculty associated with a psychology department at a PhD-granting institution in the United States.  The population contained `r format(all_people$people, big.mark = ",")` individuals as of June 2017.  All `r format(all_people$people, big.mark = ",")` members of this population were contacted via email and asked to participate.  Of the `r format(all_people$people, big.mark = ",")` email invitations sent, `r bounce_rate$bounce` emails bounced (`r (bounce_rate$bounce/all_people$people)*100`%).  We collected `r full_responses$full` full responses (`r (full_responses$full/all_people$people)*100`% full response rate), and `r partial_responses$partial` partial responses.  Only full responses are used in the following estimations.  Additionally, `r removed$removed` participant responses were removed for either being marked complete erroneously or due to breaking estimate-specific criteria.  There was no compensation offered for participation. `r female$female` (`r (female$female/full_responses$full)*100`%) participants identified as female, `r male$male` (`r (male$male/full_responses$full)*100`%) identified as male, and `r DND_gender$DND_gender` (`r (DND_gender$DND_gender/full_responses$full)*100`%) chose not to identify their gender.  `r assistant$assistant` (`r (assistant$assistant/full_responses$full)*100`%) participants identified as an Assistant Professor, `r associate$associate` (`r (associate$associate/full_responses$full)*100`%) as Associate Professor, and `r full$full` (`r (full$full/full_responses$full)*100`%) as Full Professor.  `r DND_tenure$DND_tenure` participants identified as tenure or tenure-track, but did not disclose their tenure level.

## Data Sources
Data was collected using three surveys (as opposed to the two proposed in the preregistration, see supplemental materials), designed and distributed using Qualtrics survey software [@Qualtrics].  Each survey asked questions designed to estimate the total social network size of the participant, as well as demographic questions.  Surveys 1 and 2 each contained questions appropriate for the unmatched count technique (UCT).  Survey 3 contained, instead of the UCT, our direct estimate measure and questions used to determine transmission of QRP-identity information within an individual's social network.

Participants were contacted by email and provided a link to one of the Qualtrics surveys.  Upon agreeing to the informed consent, participants completed the aforementioned measures.  To ensure the highest number of participants in our game of contacts procedure (see measures), half of the total population were asked to participate in Survey 3, which contained our direct estimate question.  Thus, `r DE_solicit$emails_sent` were solicited for Survey 3.  The remaining `r remaining$sum` psychologists contacted were asked to participate in our unmatched count estimate (Survey 1 or Survey 2), with `r ILC_participants$sum` randomized into the innocuous list group, and `r SLC_participants$sum` randomized into the concealable list group.  Participants had seven days to complete the survey upon starting.  Since data was collected between September 2017 and December 2017, questions framed "in the past 12 months" bound actual QRP use between September 2016 and December 2017, a time frame of 15 months.  Therefore, estimates of "current QRP use" are based on the number of psychologists who have used at least one QRP in this time frame.

All surveys included the definition of "Questionable Research Practices (QRPs)".  This definition included the list of behaviors previously defined in the literature as QRPs (see Table), but omitting "fabricating data" for reasons addressed earlier.  The definition of QRP was made available on each relevant question with a mouse rollover that was first demonstrated with the initial definition.  For the full text of our definition used, see supplemental materials.

Additionally, QRP use (as defined in the present work) is constrained to behaviors performed "in the past 12 months".  Although some have found instances of underreporting when using a 12 month recall [@Landen1995; @Connelly1995], this time frame is used frequently to measure current behavior in major national data collection surveys such as the National Health Interview Survey (NHIS) [@NHIS2006] and the National Survey on Drug Use and Health [@Ahrnsbrak2017].

## Measures
### Direct Estimate
The direct estimate involved asking members of the target population whether they have used at least one QRP in the past 12 months, and is calculated as follows:
\begin{equation}
\rho = \frac{c}{n}
\end{equation}
where $\rho$ is the proportion estimate of people who have used at least one QRP in the past 12 months, $c$ is the number of participants indicating they have used a QRP in the past 12 months, and $n$ is the total number of participant responses.

### Unmatched Count Technique
The unmatched count technique (UCT) is an indirect way of measuring the base rates of concealable and potentially stigmatized identities [@Wolter2014;@Gervais2017]. In this estimate, two groups of participants are given a list of innocuous items that could apply to them (e.g., I own a dishwasher; I exercise regularly). The list of items for both groups is the same except for one additional item that one group receives and the other does not.  This extra item asks about the concealable identity (e.g., I own a dishwasher; I exercise regularly; I smoke crack cocaine - examples from [@Gervais2017]). See Table \ref{tab:UCT} for the full list of items used.  Participants are asked to count and report the number of items in the list that apply to them.  At no point does a participant identify themselves with any particular list item, only the total number of applicable items.
The proportion of participants that identify with the stigmatized identity is calculated as:
\begin{equation}
\rho = \frac{\sum x_y^s}{n^s} - \frac{\sum x_y^i}{n^i}
\end{equation}
where $\rho$ is the proportion estimate of people who have used at least one QRP in the past 12 months, $x_y^s$ is the number of reported items for participant $y$ in the stigma list group $s$, $n^s$ is the total number of participant responses in group $s$, $x_y^i$ is the number of reported items for participant $y$ in the innocuous list group $i$, and $n^i$ is the total number of participants in group $i$.

### Network Methods
Network methods estimate population sizes using information about the personal networks (referred to as "ego networks" in this literature, i.e., [@McCormick2010]) of respondents, based on the assumption that personal networks are, on average, representative of the population [@Salganik2011]. Each participant's social network provides a sample of the general population, and by collecting network data on many participants, those accumulated social networks provide access to the larger population.

#### Network Scale-Up Method (NSUM)
Participants were asked about how many people they "know" in the frame population.  In this study, "know" was defined as: they know you by face or by name, you know them by face or by name, you could contact the person if you wanted to, and you've been in contact in the past two years [@Bernard2010].  Participants were then asked a series of questions to estimate the total size of their social network, and the number of people they know who have used at least one QRP in the past 12 months.  Together, the network scale-up can be used to estimate the proportion of QRP users, and was calculated as follows:
\begin{equation}
\rho = \frac{\sum y_i}{\sum d_i}
\end{equation}
where $\rho$ is the proportion estimate of people who have used at least one QRP in the past 12 months, $y_i$ is the number of people known in the target group $y$ by participant $i$, and $d_i$ is the estimated total number of people known $d$ by participant $i$ within the frame population (see [@Killworth1998a] for more on estimating $d$).  Equation 3 makes two assumptions: that members of the general frame population know all identity information about all members of their ego networks, and that QRP users have the same size social networks as the general frame population.  

```{r UCT, echo = FALSE, results = "asis"}

UCT_table <- c("I am a vegetarian", "Innocuous", "I own a dog", "Innocuous", "I work on a computer nearly every day", "Innocuous", "I have a dishwasher in my kitchen", "Innocuous", "I can drive a motorcycle", "Innocuous", "My job allows me to work from home at least once a week", "Innocuous", "I jog at least four times a week", "Innocuous", "I enjoy modern art", "Innocuous", "I have attended a professional soccer match", "Innocuous", "I have used at least one QRP in the past 12 months", "Sensitive")

UCT_table_formatted <- matrix(UCT_table, ncol = 2, byrow = TRUE) %>% 
  as_tibble()

names(UCT_table_formatted) <- c("Item", "Item Type")

UCT_table_formatted %>% 
  kable("latex", booktabs = T, caption = "Items used in the Unmatched Count Estimate.") %>%
  kable_styling(font_size = 9) %>% 
  column_spec(1, width = "6.3cm")

```

#### Generalized Network Scale-Up Method (GNSUM)
Since QRP use is concealable and potentially stigmatizing, the assumptions made for Equation 3 not hold.  For that reason, data was collected from self-identifying QRP users to estimate how QRP-use identity information transmits through ego networks.  This estimate is called the transmission rate, or tau ($\tau$).  This data was collected using the game of contacts method [@Salganik2012], described below.

To estimate the QRP use identity transmission rate, $\tau$, we performed the game of contacts with participants who self-identified as using at least one QRP in the past 12 months.  Briefly, this method has participants (called egos in network terminology) answer a set of questions about what they know about the QRP use of several others (called alters) in their social network, and what those alters know about the participant's QRP use.  The questions are semi-graphical and responses are recorded on a digital 2x2 grid, representing the four possible ways information can flow through a given ego-alter relationship. The transmission rate is then calculated as:
\begin{equation}
\tau = \frac{\sum{w_i}}{\sum{x_i}}
\end{equation}
where $w_i$ is the number of alters that know the ego is a member of the target population, and $x_i$ is the total number of alters generated by the ego.  This produced a value between 0 and 1.   For a full description of the game of contacts, see @Salganik2012. 

This study utilized a digital distribution of the game of contacts.  This method is typically performed in a face-to-face interview setting with the participant (Salganik2012b).  Due to the distributed nature of our frame population, this was not feasible.  Instead, participants were presented with the game of contacts via Qualtrics.  These questions were pretested with several academics not within the frame population.  A comparison between an in-person and digital game of contacts has been pre-registered by the authors (https://osf.io/yf4xc/) for future study.

Additionally, to relax the assumption of equal social network sizes between the general frame population and QRP users, a popularity ratio (delta, $\delta$) was calculated by dividing the average network size of QRP users by the average network size of the general frame population.

Together, $\tau$ and $\delta$ adjust the network scale-up estimate in Equation 3 into the generalized network scale-up as follows:
\begin{equation}
\rho = \frac{\sum y_i}{\sum d_i} * \frac{1}{\tau} * \frac{1}{\delta}
\end{equation}
where $\rho$ is the proportion estimate of people who have used at least one QRP in the past 12 months, $\frac{\sum y_i}{\sum d_i}$ is the network scale-up estimate (equation 3), $\tau$ is the transmission rate, and $\delta$ is the popularity ratio.  All network scale-up results are calculated using Equation 5, incorporating $\tau$ and $\delta$.

# Results

The three estimates of recent QRP use in the frame population of American tenured or tenure-track faculty are summarized in Figure \ref{fig:estimate}, and described in detail below.


```{r echo = FALSE, fig.height = 4, fig.width = 7, fig.cap = "\\label{fig:estimate}Estimates of the current prevalence of users of questionable research practices using three different estimators; the Generalized Network Scale-Up Method (GNSUM), the Direct Estimate, and the Unmatched Count Technique (UCT).  Point estimates with 95% bootstrapped confidence intervals."}
#this is the code block for Figure 1.

#turns the data into factors for graphing
estimates$estimate_type <- factor(estimates$estimate_type, levels = c("unmatched_count", "direct_estimate", "GNSUM"))

#creates figure
estimates %>% 
  ggplot(aes(x = estimate_type, y = mean)) + geom_point(size = 2.5) + coord_flip() + geom_errorbar(aes(ymin = lower, ymax = upper, width = 0)) + xlab("") + ylab("\nEstimated Prevalence of QRP Users (%)\n") + theme_apa() + theme(legend.position = "none") + geom_hline(yintercept=0, linetype = "dotted") + scale_x_discrete(labels = c("UCT Estimate", "Direct Estimate", "GNSUM Estimate")) + scale_color_manual(values=c("black", "black", "black")) + scale_y_continuous(breaks = c(-20, -10, 0, 10, 20, 30, 40, 50, 60)) + theme(axis.text.x = element_text(size = 14, colour = "black"), axis.title.y = element_text(size = 16))

#saves the figure as a png file (remove hashtag to save - it's hashed now for knitting purposes)
#ggsave("estimate_hold.png")

```

## Direct Estimate
To ensure the highest number of participants in our game of contacts, half of the total population were asked to participate in Survey 3, which contained our direct estimate question. Thus, `r format(DE_solicit$emails_sent, big.mark = ",")` psychologists were solicited, and we received `r DE_received_responses$analyzed_participants` responses to Survey 3 able to be analyzed. Of the `r DE_received_responses$analyzed_participants` participants, `r DE_numerator$QRP` indicated they had used at least one QRP in the past 12 months.  Using Equation 1, we calculated QRP prevalence to be `r (boot_DE$t0)*100`% (bootstrapped 95% confidence interval [`r boot_DE_ci$percent[4]*100`%, `r boot_DE_ci$percent[5]*100`%]).  This corresponds to an estimated `r format(round(all_people$people*boot_DE$t0, 0), big.mark = ",")` American psychologists currently using QRPs.

It is possible this estimate underestimates the true number of psychologists using QRPs.  For one, social desirability may lead some scientists who have used QRPs to be unwilling to admit it.  This estimate is only generated by those participants willing to reveal their identity as a QRP user.  Given the somewhat critical social environment for QRP users [@Fiske2016;@TeixeiradaSilva2018], it is reasonable to believe some participants withheld their identity when we asked directly.  The following indirect estimation methods sought to mitigate this social desirability bias.

## Unmatched Count Technique
The remaining `r format(remaining$sum, big.mark = ",")` psychologists contacted were asked to participate in our unmatched count estimate with `r format(ILC_participants$sum, big.mark = ",")` randomized into the innocuous list condition, and `r format(SLC_participants$sum, big.mark = ",")` randomized into the sensitive list condition.

The average number of list items corresponding to participants in the innocuous list condition was `r ILC_average$mean`.  The average number of list items corresponding to participants in the sensitive list condition was `r SLC_average$mean`.  Using Equation 2, we calculated QRP user prevalence to be `r boot_UCTE$t0*100`% [`r boot_UCTE_ci$percent[4]*100`%, `r boot_DE_ci$percent[5]*100`%].  This corresponds to an estimated `r round(all_people$people*boot_UCTE$t0, 0)` American psychologists currently using QRPs. 

It was unexpected that the calculated UCT estimate would be lower than our direct estimate.  Typically, due to reducing response bias, UCT estimates are larger than direct estimates when the behavior or identity in question is concealable and potentially stigmatized [@Gervais2017; @Wolter2014;@Starosta2014].  Given the bootstrapped 95% confidence interval crosses zero, it is likely the relatively low number of participants in our UCT (n = `r UCT_total_participants$sum`) led this calculation to be overly sensitive to individual responses, and as such, we do not consider this estimate to be valid or accurate.

## Generalized Network Scale-Up Estimate
All participants who were randomized into the UCT estimate were also asked to answer questions about their social networks, and to estimate how many researchers they know who have used at least one QRP in the past 12 months.  Participants who were randomized into the direct estimate and who self-identified as a QRP user in that estimate were also asked to answer questions about their social network and to participate in the game of contacts method.  Participants in the direct estimate who did not self-identify as a QRP user were asked questions about their social network as well, but were not asked how many researchers they know who have used at least one QRP in the past 12 months. Therefore, we collected social network responses from `r SN_responses$total` participants from the general frame population (to be used in estimating $\delta$), `r DE_numerator$QRP` responses from participants who self-identified as QRP users who also completed the game of contacts (to be used in estimating $\tau$), and `r UCT_total_participants$sum` responses from participants who estimated the number of researchers they know who have used at least one QRP in the past 12 months.

These `r UCT_total_participants$sum` identified a sum total of `r y_i$estimate` QRP users, and know a sum total of `r format(round(d_i$estimate, 0), big.mark = ",")` researchers.  Given the total frame population is `r format(all_people$people, big.mark = ",")`, we are fairly confident all or nearly all members were identified at least once by our participants.  Using the network scale-up in Equation 3, this generates an estimate of `r boot_NSUME$t0*100`% [`r boot_NSUME_ci$percent[4]*100`%, `r boot_NSUME_ci$percent[5]*100`%].  This estimate serves as the base starting point our key network estimate, the Generalized Network Scale-Up Estimator, detailed below.

Equation 5 relaxes the assumptions of equal network size and total information transmission by incorporating $\tau$ and $\delta$.  Using the `r SN_responses$total` responses from the general population and the `r DE_numerator$QRP` responses from the participants who indicated using a QRP in the past 12 months, we estimate $\delta$ as `r delta$mean`.  Using the game of contacts, we estimate $\tau$ as `r  tau`. Using Equation 5, we estimate QRP user prevalence to be `r boot_GNSUME$t0*100`% [`r boot_GNSUME_ci$percent[4]*100`%, `r boot_GNSUME_ci$percent[5]*100`%].  This corresponds to an estimated `r format(round(all_people$people*boot_GNSUME$t0, 0), big.mark = ",")` American psychologists currently using QRPs.

To assess the accuracy of participants in estimating the size of this unknown group (QRP users), we generated additional estimates of `r length(validity$name)` populations of known size; the number of psychologists with particular first names (the number of psychologists named David, named Janet, etc).  The `r length(validity$name)` names were gender balanced and represented common, uncommon, and rare names that exist within the census of the frame population.  The size estimates of these populations of known size can be seen in Figure \ref{fig:validity}.  The estimates made by our participants of the size of these `r length(validity$name)` populations seem reasonable and closely mirror the actual prevalence of these groups.  The correlation between our participant's estimate of those group sizes and the actual group sizes is r = `r round(validity_correlation, 2)`.  The fact that the same estimator in the same group of participants can generate reasonable estimates for populations of known size is encouraging evidence of the accuracy of our estimate of the number of recent QRP users utilizing the generalized network scale-up estimate.

```{r echo = FALSE, collapse = TRUE, fig.height = 5, fig.width = 5, fig.cap="\\label{fig:validity}Validation of Network Scale-Up Estimates using 24 groups of known size.  Each point represents one group, with 95% confidence intervals.  Dotted line represents when estimated group prevalence equals actual group prevalence. Correlation between estimated and actual group prevalence, r = 0.91."}
#this is the code block for Figure 2.

#graphs validity scatterplot
validity %>% 
  ggplot(aes(x = actual, y = estimate)) + geom_point(size = 1) + geom_errorbar(aes(ymin = (estimate - CI), ymax = (estimate + CI))) + theme_apa() + geom_abline(linetype = "dotted") + ylim(0, 3) + xlim(0, 3) + ylab("Estimated Prevalence of Group (%)") + xlab("Actual Prevalence of Group (%)")

#saves the figure as a png file (remove hashtag to save - it's hashed now for knitting purposes)
#ggsave("validity_hold.png")

```

# DISCUSSION
Because of inconsistencies in previous research, this study generated three estimates of current QRP use,  using three independent estimating procedures.  Depending on the estimator used, we estimate `r boot_DE$t0*100`% to `r boot_GNSUME$t0*100`% of American psychologists currently use questionable research practices.  Our unmatched count estimate produced an estimate of `r boot_UCTE$t0*100`%, though this may not be a valid or accurate estimate.

To the best of our knowledge, this is the first report of the prevalence of QRP users in a proximal timespan.  As such, it is difficult to draw conclusions about the magnitude of our estimates when compared to previous estimates. 

Compared to @John2012 and @Agnoli2017, we estimate lower rates of questionable research practices.  Compared to @Fiedler2016, however, we estimate higher rates of these practices.  Our definition of "questionable research practices" were the same ones used in @John2012 and @Agnoli2017, but was restricted to a timespan of only 15 months, so it is reasonable that our estimates would be lower than those with an unrestricted timeframe of QRP use.  Since we used those same QRP definitions, is also reasonable that our estimates would be higher than those described by @Fiedler2016, who changed the definitions of each QRP.

This is also the first report to use the generalized network-scale up estimators to investigate the prevalence of QRP users in psychology.  Direct estimates rely on an individual's willingness to participate and their willingness to honestly share their identity as a QRP user.  Bias in either of these dimensions can distort a direct estimate.

Social network methods, on the other hand, enable researchers to better understand the social processes at work that produce an environment where members vary in their identity and the information they share with others [@Zheng2006].  In the process of producing a population size estimate for current QRP users, we also reported the first estimate of the social transmission of this QRP identity - tau ($\tau$).

Our reported estimate of $\tau$ (`r tau` or `r  tau*100`%) means very few individuals who use QRPs share this identity with others, choosing instead to conceal this identity from their peers.  Because of this, it is important to shift the current conversation on QRPs away from isolated behaviors (i.e., prevalence of rounding down p values) and towards the usuers of these behaviors (i.e., prevalence of people who round down p values).  Re-framing QRP prevalence away from the behavior and towards the users brings our field-wide problems more into scope with existing literature on concealable identities and stigma.  For example, much work has been done focusing on how increasing stigma inadvertently locks individuals into detrimental behaviors [@Stuber2009;@Bayer2002], and how revealing a concealed identity can increase well-being by reducing the stress of being exposed [@Chaudoir2010]. Framing QRP use in terms of the individual may help the field reduce QRP use by increasing awareness of the effects of stigma and support.

## Implications
These estimates serve as a baseline to measure the effectiveness of current initiatives, as well as a foundation for new ones.  While much work is being done to grow support for interventions such as pre-registration [@Wagenmakers2016] and Registered Reports [@Chambers2014], it is currently unknown what quantitative effect these are having on curbing behaviors associated with inflated Type I error such as QRPs.  By performing follow-up estimates at future time points, the field can use the baseline estimates presented here to measure the effectiveness of these programs at reducing QRP use.

## Limitations & Future Directions
Our unmatched count estimate was lower than our direct estimate and had a confidence interval that included zero, which we did not expect.  This estimate is computed as a difference between averages, which means each is sensitive to the number of participants.  Our innocuous item group had `r ILC_total$analyzed_participants`, and our concealable item group had `r SLC_total$analyzed_participants`.  As denominators in computing the means needed for Equation 2, these are low.  For example, if one additional participant in the innocuous list condition responded by itentifying with 6 of 9 items (1 standard deviation above the mean), our UCT estimate would change from 10.46% to 9.15%.  This is reflected in the bootstrapped confidence interval crossing zero, indicative of an unstable difference between the innocuous and concealable list groups.  Future work using the UCT would benefit from larger sample sizes, as demonstrated in @Gervais2017.

As noted in the introduction, QRPs exist in a grey area of accepted scientific practice.  Therefore, it is difficult to interpret the severity of QRP use.  This difficulty, along with the high variability among previous estimates of QRP prevalence, has led to a number of different conclusions.  Some have concluded that the problems are overstated [@Fanelli2018], while others argue QRP use presents a real threat to the viability of several scientific fields, such as education and political science [@Bosco2016].  Although our work moves the field forward in understanding the prevalence of those that use these behaviors, it provides less guidance on the severity of the consequences of QRP use on the whole.

Science is a globally distributed network, and as such, can be difficult to study.  Our reported estimates were limited to American psychologists, though we know that these issues are not restricted solely to the United States [@Forsberg2018;@Fiedler2016;@Agnoli2017]. Future studies estimating the prevalence of QRP use in other countries will be an important next step, as will investigating the use of QRPs in other scientific fields.  Some of this work has already started through the Horizon 2020 framework in the European Union [@Forsberg2018], though more innovative work will be required to better understand the scope of the problems faced.

## Conclusion
By directly asking participants about their use of QRPs, we estimate `r boot_DE$t0*100`% have used at least one QRP in the past 12 months. The generalized network scale up estimate is `r boot_GNSUME$t0*100`%, which corresponds to between `r format(round(boot_DE$t0*all_people$people, 0), big.mark = ",")` and `r format(round(boot_GNSUME$t0*all_people$people, 0), big.mark = ",")` American psychologists.  Although some have argued the narrative of the "replication crisis" is overblown [@Fanelli2018], the current work illustrates how common QRP use is. Although many have called for changes in statistical inference practices to mitigate false-positive findings [@Lakensabc1860; @Benjamin2017], it is important that we as a field also focus on disincentivizing the use of questionable research practices (and other behavioral degrees of freedom) among our peers and coworkers for the betterment of our science.

\newpage



# References
```{r create_r-references, echo = FALSE}
r_refs(file = "library.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup